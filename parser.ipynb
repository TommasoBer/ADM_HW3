{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import csv\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import heapq \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "This function **scrap_book** is the main core of our scraping part.\n",
    "Using the \"BeautifulSoup\" library, *scrap_book* takes as input the html page in soup_format and scrapes the different data that we need to build our dataset.\n",
    "\n",
    "As others input, there is **nlp** that is a tool for the Natural Language Processing and **n_link** that is useful to retrive the book's Url from the list previopus created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_book(page_soup , nlp , n_link):\n",
    "    \n",
    "    n_link = int(n_link)\n",
    "    n_link = n_link-1\n",
    "    \n",
    "    #Scrap the book's title\n",
    "    book_title = page_soup.find_all('h1', id =\"bookTitle\")[0].contents[0].replace('\\n', '').strip()\n",
    "    \n",
    "    print(book_title)\n",
    "    \n",
    "    # Scrap the book's serie\n",
    "    series = page_soup.find_all('h2', id=\"bookSeries\")[0].contents[0].replace('\\n', '').strip() \n",
    "    \n",
    "    #Scrap the book's author\n",
    "    author = page_soup.find_all('span', itemprop='name')[0].contents[0].replace('\\n', '').strip()\n",
    "    \n",
    "    #Scrap the rating value\n",
    "    rating_value = page_soup.find_all('span', itemprop='ratingValue')[0].contents[0].replace('\\n', '').strip()\n",
    "    \n",
    "    #Scrap the Rating_Count and Review_Count\n",
    "    ratings = page_soup.find_all('a', href=\"#other_reviews\")\n",
    "    rating_count = -1\n",
    "    rating = -1\n",
    "    for raiting in ratings:\n",
    "        if raiting.find_all('meta', itemprop=\"ratingCount\"):\n",
    "            rating_count = raiting.text.replace('\\n', '').strip().split(' ')[0]\n",
    "        elif raiting.find_all('meta', itemprop=\"reviewCount\"):\n",
    "            review_count = raiting.text.replace('\\n', '').strip().split(' ')[0]\n",
    "    \n",
    "    \n",
    "    #Scrap the Plot\n",
    "    description = ' '.join([c for c in page_soup.find_all('span', id = re.compile(r'freeText\\d'))[0].contents \\\n",
    "                           if isinstance(c, str)])\n",
    "     \n",
    "    \n",
    "    doc = nlp(description)\n",
    "    token_list_plot = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    \n",
    "    #Scrap number of pages\n",
    "    n_pages = page_soup.find_all('span', itemprop='numberOfPages')[0].contents[0].strip().split(' ')[0]\n",
    "    \n",
    "    #Scrap Publishing date of the book\n",
    "    pub_date = page_soup.find_all('div', {\"class\": \"row\"})[1].contents[0].strip().split(' ')\n",
    "    pub_date = \" \".join(pub_date)\n",
    "    pub_date = pub_date.replace('\\n', '').strip()\n",
    "    pub_date = pub_date.replace(\" \",\"\")\n",
    "    pub_date = pub_date.split('d')[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Scrap Characters\n",
    "   \n",
    "    Characters = []\n",
    "    lenght_c = page_soup.find_all('a', {'href': re.compile(r'/characters/')})\n",
    "    l = len(lenght_c)\n",
    "    for i in range (0, l):\n",
    "        character = page_soup.find_all('a', {'href': re.compile(r'/characters/')})[i].contents[0]\n",
    "        Characters.append(character)\n",
    "        \n",
    "        \n",
    "    #Scrap Settings\n",
    "    Settings = []\n",
    "    lenght_s = page_soup.find_all('a', {'href': re.compile(r'/places/')})\n",
    "    l2 = len(lenght_s)\n",
    "    for i in range (0, l2):\n",
    "        setting = page_soup.find_all('a', {'href': re.compile(r'/places/')})[i].contents[0]\n",
    "        Settings.append(setting)    \n",
    "    Settings = \" \".join(Settings)\n",
    "    \n",
    "    \n",
    "    #scrap original book's link\n",
    "    f=open('lista_url.txt')\n",
    "    lines=f.readlines()\n",
    "    URL = lines[n_link]\n",
    "    \n",
    "\n",
    "    return book_title,series,author,rating_value,rating_count, \\\n",
    "            review_count,token_list_plot,n_pages,pub_date,Characters,Settings,URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------\n",
    "\n",
    "This is the script that we launch in order to extract information from the html pages and store them in the file \"articles.tsv\". \n",
    "\n",
    "**filepath** : It is the forder in which there are the .html pages previous downloaded\n",
    "\n",
    "**articles.tsv** : It 's the file that we build. Using the function *scrap_book* we extract info about each page/book and put them in this file, in order to build our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = r\"C:\\Users\\thoma\\Desktop\\HW3_ADM\\html_folder3\\articoli_7\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "with open('articles.tsv', 'a', encoding=\"utf-8\") as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    #tsv_writer.writerow(['bookTitle', 'bookSeries','bookAuthors','ratingValue','ratingCount','reviewCount','Plot','NumberofPages','Published','characters','Settings','URL'])\n",
    "\n",
    "    for i in range (24001,30001):\n",
    "        try :\n",
    "            print(i)\n",
    "\n",
    "            string1 = \"article_\"\n",
    "            string2 = str(i)\n",
    "            string3 = \".html\"\n",
    "            title = string1 + string2 + string3\n",
    "\n",
    "            soup = BeautifulSoup(open(os.path.join(filepath, title),encoding='utf-8' ), features=\"lxml\")\n",
    "\n",
    "\n",
    "            bookTitle,bookSeries,bookAuthors,ratingValue,ratingCount, reviewCount, Plot, \\\n",
    "            NumberofPages,Published, characters,Settings,URL = scrap_book(soup, nlp,string2)\n",
    "\n",
    "            riga_info = (bookTitle,bookSeries,bookAuthors,ratingValue,ratingCount, reviewCount, \\\n",
    "                         Plot, NumberofPages,Published,characters,Settings,URL)\n",
    "\n",
    "\n",
    "            tsv_writer.writerow([riga_info[0],riga_info[1],riga_info[2],riga_info[3],riga_info[4],riga_info[5], \\\n",
    "                                 riga_info[6],riga_info[7],riga_info[8],riga_info[9],riga_info[10],riga_info[11]])\n",
    "        \n",
    "        #For some reason related to the differents pages of the website could be an error in scrping process: we handle it.\n",
    "        except IndexError as e :\n",
    "            print(\" lost a book because of scraping error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
