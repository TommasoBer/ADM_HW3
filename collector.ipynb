{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import csv\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import heapq \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part_1 -  Data Collection\n",
    "\n",
    "\n",
    "### 1.1. Get the list of books\n",
    "\n",
    "### Scrap the links in a page containing list of books\n",
    "\n",
    "Thanks to this function, for each page that arrive in input as href, we are able to scrap the book's Urls in that. N.B: each page has 100 books, so we have to find 100 links for page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_url(href, driver , nlp):\n",
    "    driver.get(href)       \n",
    "    #time.sleep(5)\n",
    "    page_soup = BeautifulSoup(driver.page_source, features=\"lxml\")\n",
    "    \n",
    "    links = page_soup.find_all('a' , itemprop=\"url\")\n",
    "    \n",
    "    lista_links = []\n",
    "    \n",
    "    i = 2\n",
    "    for link in links:\n",
    "        link_full = link.get('href')\n",
    "        if (i % 2) == 0 :\n",
    "            string1 = 'https://www.goodreads.com/en'\n",
    "            link_full = string1 + link_full\n",
    "            lista_links.append(link_full)\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    return lista_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the book's links\n",
    "\n",
    "Thanks to this script we are able to read the web-pages that contain the list of best books on the web-site \"https://www.goodreads.com\" and, for each page, take the Url of the book we are interested in, saving them in a file called \"lista_url.txt\". In each page, we will scrap the links of 100 books\n",
    "\n",
    "**lista_url.txt** : This file will contain 30k rows, each row is the link of a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chromedriver = r\"C:\\Users\\thoma\\Desktop\\HW3_ADM\\chromedriver_win32\"\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "for i in range (1, 301) :\n",
    "    href = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"\n",
    "    \n",
    "    stringa2 = i\n",
    "    stringa2 = str(stringa2)\n",
    "    href = href + stringa2\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    #Let's use the fuction able to find the urls in the page.\n",
    "    urls = scrap_url(href,driver,nlp)\n",
    "    \n",
    "    with open('lista_url.txt', 'a') as f:\n",
    "        for item in urls:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl books\n",
    "\n",
    "### Download the HTML pages\n",
    "\n",
    "With this function we are able to save in a folder \"html_folder\" all the books as .html page.\n",
    "It takes the page, as link from \"lista_url.txt\", and download it savingig as .html file in the setted folder/filepath.\n",
    "After this step, we have all the book's html-pages downloaded, and we are ready to scrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = r'C:\\Users\\thoma\\Desktop\\HW3_ADM\\html_folder'\n",
    "\n",
    "#Da lanciare come sta settato ora per prendere nuovi file\n",
    "\n",
    "with open(\"lista_url.txt\") as file_in:\n",
    "    \n",
    "    line_count = 1    # article_i at i-th row of \"lista_url.txt\"\n",
    "    \n",
    "    for link in islice(file_in, 1, 30000):\n",
    "        \n",
    "        page = requests.get(link, allow_redirects=True)\n",
    "        contenuto = page.text\n",
    "        soup = BeautifulSoup (contenuto , features = 'lxml')\n",
    "        \n",
    "        \n",
    "        if line_count == 30002 :\n",
    "            break\n",
    "        string1 = \"article_\"\n",
    "        string2 = str(line_count)\n",
    "        string3 = \".html\"\n",
    "        title = string1 + string2 + string3   \n",
    "        # We automatically assign to each book the name \"article_i.html\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        with open (os.path.join(filepath, title), \"w\",encoding='utf-8' ) as f2:\n",
    "            f2.write(str(soup))\n",
    "        line_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
